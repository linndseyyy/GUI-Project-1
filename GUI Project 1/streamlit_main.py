import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import pickle
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import metrics
import seaborn as sns
from product_search_service import search_product_by_code, print_product_info

# 1. Read data
# Change to utf-8 for vietnamese text
data = pd.read_csv("final_data3.csv", encoding='utf-8')
# Load d·ªØ li·ªáu b√¨nh lu·∫≠n ri√™ng
comment_data_path = "Danh_gia2.csv"
comment_data = pd.read_csv(comment_data_path, encoding='utf-8')

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV2 (ch·ª©a c√°c ƒë√°nh gi√° t√≠ch c·ª±c/ti√™u c·ª±c)
filtered_path = '/Users/nii/Desktop/GUI Project 1 copy/resources/Data for GUI.csv'
filtered = pd.read_csv(filtered_path)

# 2. Data pre-processing
source = data['noi_dung_binh_luan']
target = data['label']

# Clean NaN values by replacing them with empty string
source = source.fillna('')

text_data = np.array(source)

count = CountVectorizer(max_features=6000)
count.fit(text_data)
bag_of_words = count.transform(text_data)

X = bag_of_words.toarray()

y = np.array(target)

#3. Save models
  
# luu model CountVectorizer (count)
pkl_count = "count_model.pkl"  
with open(pkl_count, 'wb') as file:  
    pickle.dump(count, file)

#4. Load models 
# ƒê·ªçc model
# import pickle
pkl_filename = "lg_predictor.pkl"  
with open(pkl_filename, 'rb') as file:  
    lg_prediction = pickle.load(file)
# Read count len model
with open(pkl_count, 'rb') as file:  
    count_model = pickle.load(file)

#--------------
# GUI
st.title("Sentiment Analysis Project")
st.write("## Hasaki - ƒê√°nh gi√° t√≠ch c·ª±c v√† ti√™u c·ª±c")

menu = ["Business Objective", "Build Project", "New Prediction", "Product Search for Customers", "Product Search for Business Owners"]
choice = st.sidebar.selectbox('Menu', menu)
st.sidebar.write("""#### Th√†nh vi√™n th·ª±c hi·ªán:
                 L√™ Gia Linh & Ph·∫°m T∆∞·ªùng H√¢n""")
st.sidebar.write("""#### Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: Khu·∫•t Thu·ª≥ Ph∆∞∆°ng""")
st.sidebar.write("""#### Th·ªùi gian th·ª±c hi·ªán: 12/2024""")
if choice == 'Business Objective':  
    st.subheader("Gi·ªõi thi·ªáu doanh nghi·ªáp")  
    st.write("""Hasaki.vn - m·ªôt h·ªá th·ªëng c·ª≠a h√†ng m·ªπ ph·∫©m ch√≠nh h√£ng v√† d·ªãch v·ª• chƒÉm s√≥c s·∫Øc ƒë·∫πp chuy√™n s√¢u v·ªõi m·∫°ng l∆∞·ªõi r·ªông kh·∫Øp Vi·ªát Nam. V·ªõi m·ªôt h·ªá th·ªëng website cho ph√©p kh√°ch h√†ng ƒë·∫∑t h√†ng v√† ƒë·ªÉ l·∫°i b√¨nh lu·∫≠n, Hasaki.vn c√≥ ƒë∆∞·ª£c m·ªôt c∆° s·ªü d·ªØ li·ªáu kh√°ch h√†ng l·ªõn v√† ƒë·∫ßy ti·ªÅm nƒÉng khai th√°c.
    """)  
    st.write(""" Gi·ªõi h·∫°n hi·ªán t·∫°i: D·ªØ li·ªáu ƒë√°nh gi√° l√† vƒÉn b·∫£n th√¥ v√† ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω m·ªôt c√°ch t·ª± ƒë·ªông ‚áí Y√™u c·∫ßu qu√° tr√¨nh ph√¢n t√≠ch th·ªß c√¥ng, t·ªën th·ªùi gian v√† d·ªÖ x·∫£y ra sai s√≥t.
    """)  
    st.image("Hasaki.jpg")
    st.subheader("Business Objective")
    st.write("""
    ###### X√¢y d·ª±ng h·ªá th·ªëng/m√¥ h√¨nh d·ª± ƒëo√°n nh·∫±m: 
            1. Ph√¢n lo·∫°i c·∫£m x√∫c c·ªßa kh√°ch h√†ng d·ª±a tr√™n c√°c ƒë√°nh gi√° (Positive, Neutral, Negative).
        2. TƒÉng t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c trong vi·ªác ph·∫£n h·ªìi √Ω ki·∫øn c·ªßa kh√°ch h√†ng.
        3. H·ªó tr·ª£ Hasaki.vn v√† c√°c ƒë·ªëi t√°c c·∫£i thi·ªán s·∫£n ph·∫©m, d·ªãch v·ª•, n√¢ng cao s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng.
    """)  
    st.write("""###### Y√™u c·∫ßu: D√πng thu·∫≠t to√°n Machine Learning algorithms trong Python ƒë·ªÉ ph√¢n lo·∫°i b√¨nh lu·∫≠n t√≠ch c·ª±c, trung t√≠nh v√† ti√™u c·ª±c.""")
    st.image("Sentiment Analysis.jpg")

elif choice == 'Build Project':
    st.subheader("Build Project")
    st.write("##### 1. M·ªôt v√†i d·ªØ li·ªáu")
    st.dataframe(data[['ma_san_pham','noi_dung_binh_luan', 'label']].head(3))
    st.dataframe(data[['ma_san_pham','noi_dung_binh_luan', 'label']].tail(3))  

    st.write("##### 2. Tr·ª±c quan ho√° Sentiment Analysis")
    st.write("###### Wordcloud b√¨nh lu·∫≠n")
    st.image("Wordcloud.png")
    st.write("###### Ki·ªÉm tra s·ª± c√¢n b·∫±ng d·ªØ li·ªáu")
    st.image("Plot 1.png")
    st.write("""###### ‚áí D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng, c·∫ßn th·ª±c hi·ªán oversample ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu""")
    st.write("""###### Sau khi th·ª±c hi·ªán c√¢n b·∫±ng d·ªØ li·ªáu""")
    st.image("Plot 2.png")
   
    st.write("##### 3. X√¢y d·ª±ng m√¥ h√¨nh")
    st.write("""X√¢y d·ª±ng m·ªôt m√¥ h√¨nh s·ª≠ d·ª•ng ƒëa d·∫°ng c√°c thu·∫≠t to√°n g·ªìm Naive Bayes, Logistic Regression v√† Random Forest. C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c ƒë√°nh gi√° c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m tr√™n website Hasaki.vn ƒë·ªÉ ph√¢n lo·∫°i th√†nh c√°c m·ª©c ƒë·ªô c·∫£m x√∫c.""")

    st.write("##### 4. ƒê√°nh gi√°")
    st.write("""X√¢y d·ª±ng m·ªôt m√¥ h√¨nh s·ª≠ d·ª•ng ƒëa d·∫°ng c√°c thu·∫≠t to√°n g·ªìm Naive Bayes, Logistic Regression v√† Random Forest. C√°c m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n c√°c ƒë√°nh gi√° c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m tr√™n website Hasaki.vn ƒë·ªÉ ph√¢n lo·∫°i th√†nh c√°c m·ª©c ƒë·ªô c·∫£m x√∫c.""")
    st.write("""###### ƒê·ªô ch√≠nh x√°c v√† th·ªùi gian ch·∫°y model""")
    st.image("Model Performance.png") 
    st.write("""###### Confusion Matrix""")
    st.image("Confusion matrix.png")
    st.write("##### 5.K·∫øt lu·∫≠n: ")
    st.write("###### M√¥ h√¨nh Logistic Regression ph√π h·ª£p nh·∫•t ƒë·ªëi v·ªõi Sentiment Analysis c·ªßa t·∫≠p d·ªØ li·ªáu c·ªßa Hasaki.vn.")

elif choice == 'New Prediction':
    st.subheader("Select data")
    flag = False
    lines = None
    type = st.radio("Upload data or Input data?", options=("Upload", "Input"))
    if type=="Upload":
        # Upload file
        uploaded_file_1 = st.file_uploader("Choose a file", type=['txt', 'csv'])
        if uploaded_file_1 is not None:
            lines = pd.read_csv(uploaded_file_1, header=None)
            st.dataframe(lines)            
            lines = lines[0]     
            flag = True                          
    if type=="Input":        
        content = st.text_area(label="Input your content:")
        if content!="":
            lines = np.array([content])
            flag = True
    
    if flag:
        st.write("Content:")
        if len(lines)>0:
            st.code(lines)        
            x_new = count_model.transform(lines)        
            y_pred_new = lg_prediction.predict(x_new)       
            st.code("New predictions (0: Negative, 1. Neutral, 2: Positive): " + str(y_pred_new))


# Add Product Search for Business Owner
elif choice == "Product Search for Business Owners":
    st.subheader("Product Search for Business Owners")

    # L·ª±a ch·ªçn m√£ s·∫£n ph·∫©m
    product_ids = comment_data['ma_san_pham'].unique()
    selected_product = st.selectbox("Ch·ªçn m√£ s·∫£n ph·∫©m", product_ids)
    # L·ªçc d·ªØ li·ªáu t·ª´ CSV1 theo Product ID ƒë√£ ch·ªçn
    filtered_data = comment_data[comment_data['ma_san_pham'] == selected_product]

    # L·ªçc d·ªØ li·ªáu t·ª´ CSV2 theo Product ID t∆∞∆°ng ·ª©ng
    filtered_review_data = filtered[filtered['ma_san_pham'] == selected_product]


    # Ki·ªÉm tra v√† chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ng√†y th√°ng n·∫øu c·∫ßn
    if 'ngay_binh_luan' in comment_data.columns:
        comment_data['ngay_binh_luan'] = pd.to_datetime(comment_data['ngay_binh_luan'], errors='coerce')

        # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu NaT n√†o kh√¥ng
    
    else:
        st.warning("D·ªØ li·ªáu kh√¥ng c√≥ c·ªôt ng√†y th√°ng.")
        date_range = None

    # L·ªçc d·ªØ li·ªáu theo m√£ s·∫£n ph·∫©m
    filtered_data = comment_data[comment_data['ma_san_pham'] == selected_product]

    # B·ªô ch·ªçn th·ªùi gian: Chia th√†nh 2 ph·∫ßn - ng√†y b·∫Øt ƒë·∫ßu v√† ng√†y k·∫øt th√∫c
    if 'ngay_binh_luan' in comment_data.columns:
        st.write("Ch·ªçn kho·∫£ng th·ªùi gian:")
        
        # Ch·ªçn ng√†y b·∫Øt ƒë·∫ßu
        start_date = st.date_input(
            "Ng√†y b·∫Øt ƒë·∫ßu", 
            min_value=comment_data['ngay_binh_luan'].min(),
            max_value=comment_data['ngay_binh_luan'].max()
        )
        
        # Ch·ªçn ng√†y k·∫øt th√∫c
        end_date = st.date_input(
            "Ng√†y k·∫øt th√∫c", 
            min_value=start_date,  # Ng√†y k·∫øt th√∫c kh√¥ng ƒë∆∞·ª£c nh·ªè h∆°n ng√†y b·∫Øt ƒë·∫ßu
            max_value=comment_data['ngay_binh_luan'].max()
        )
    else:
        st.warning("D·ªØ li·ªáu kh√¥ng c√≥ c·ªôt ng√†y th√°ng.")
        start_date, end_date = None, None
   # T·ªïng s·ªë b√¨nh lu·∫≠n trong kho·∫£ng th·ªùi gian ƒë√£ ch·ªçn
    # T·ªïng s·ªë b√¨nh lu·∫≠n t·ª´ start_date ƒë·∫øn end_date
    st.write(f'#### ƒê√°nh gi√° t·ªïng quan v·ªÅ s·∫£n ph·∫©m {start_date} ƒë·∫øn {end_date}')
    total_comments = len(filtered_data)
    st.metric(f"T·ªïng s·ªë b√¨nh lu·∫≠n", total_comments)

    # S·ªë sao trung b√¨nh t·ª´ start_date ƒë·∫øn end_date
    average_star_rating = filtered_data['so_sao'].mean()

    # Ki·ªÉm tra gi√° tr·ªã NaN cho s·ªë sao trung b√¨nh
    if not pd.isna(average_star_rating):
        st.metric(f"S·ªë sao trung b√¨nh", f"‚≠ê{round(average_star_rating, 2)}/5.0")
    else:
        st.metric(f"S·ªë sao trung b√¨nh", "N/A")

    # Hi·ªÉn th·ªã c√°c b√¨nh lu·∫≠n theo s·ªë sao
    star_counts = filtered_data['so_sao'].value_counts().sort_index()
    st.bar_chart(star_counts)
    # Bi·ªÉu ƒë·ªì c·ªôt v·ªÅ s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n
    if 'ngay_binh_luan' in filtered_data.columns:
        comment_counts = filtered_data.groupby(filtered_data['ngay_binh_luan'].dt.date).size()
        fig, ax = plt.subplots()
        comment_counts.plot(kind='bar', ax=ax, color='skyblue')
        ax.set_title("S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo ng√†y")
        ax.set_xlabel("Ng√†y")
        ax.set_ylabel("S·ªë b√¨nh lu·∫≠n")
        plt.xticks(rotation=45)
        st.pyplot(fig)
    st.markdown("<hr style='border: 1px solid green;'>", unsafe_allow_html=True)

    # Hi·ªÉn th·ªã b√¨nh lu·∫≠n theo s·ªë sao
     # S·ª≠ d·ª•ng thanh slider ƒë·ªÉ l·ª±a ch·ªçn s·ªë sao
    st.write(f'#### Hi·ªÉn th·ªã b√¨nh lu·∫≠n v·ªÅ s·∫£n ph·∫©m theo s·ªë sao t·ª´ ng√†y {start_date} ƒë·∫øn {end_date}')
    star_choice = st.slider(
        "Ch·ªçn s·ªë sao ƒë·ªÉ hi·ªÉn th·ªã b√¨nh lu·∫≠n",
        min_value=1,
        max_value=5,
        value=1,  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh
        step=1,
        help="Ch·ªçn s·ªë sao t·ª´ 1 ƒë·∫øn 5 ƒë·ªÉ xem b√¨nh lu·∫≠n t∆∞∆°ng ·ª©ng"
    )

    # Hi·ªÉn th·ªã b√¨nh lu·∫≠n theo s·ªë sao ƒë√£ ch·ªçn
    st.write(f"B√¨nh lu·∫≠n {star_choice} sao")

    # L·ªçc d·ªØ li·ªáu cho s·ªë sao hi·ªán t·∫°i
    star_comments = filtered_data[filtered_data['so_sao'] == star_choice][['ma_khach_hang', 'ngay_binh_luan', 'noi_dung_binh_luan']]

    # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ d·ªÖ hi·ªÉu
    star_comments = star_comments.rename(columns={
        'ma_khach_hang': 'M√£ kh√°ch h√†ng',
        'ngay_binh_luan': 'Th·ªùi gian b√¨nh lu·∫≠n',
        'noi_dung_binh_luan': 'B√¨nh lu·∫≠n'
    })

    # M·ªü r·ªông chi·ªÅu r·ªông b·∫£ng b√¨nh lu·∫≠n (100% chi·ªÅu r·ªông c·ªßa m√†n h√¨nh)
    st.write(star_comments.style.set_table_styles([
        {'selector': 'table', 'props': [('width', '100%')]}  # ƒê·∫∑t b·∫£ng chi·∫øm to√†n b·ªô chi·ªÅu r·ªông
    ]))

    # Hi·ªÉn th·ªã b·∫£ng c√°c b√¨nh lu·∫≠n
    st.dataframe(star_comments)
    st.markdown("<hr style='border: 1px solid green;'>", unsafe_allow_html=True)

    # Hi·ªÉn th·ªã c·ª• th·ªÉ c√°c ƒë√°nh gi√°
    st.write(f"#### ƒê√°nh gi√° t√≠ch c·ª±c v√† ti√™u c·ª±c cho s·∫£n ph·∫©m")
    positive_negative_counts = [
        int(filtered_review_data['total_positive']),
        int(filtered_review_data['total_negative'])
    ]
    labels = ['T√≠ch c·ª±c', 'Ti√™u c·ª±c']
    colors = ['#2ecc71', '#e74c3c']

    fig, ax = plt.subplots(figsize=(6, 4))
    ax.pie(positive_negative_counts, labels=labels, autopct='%1.1f%%', colors=colors)
    plt.title(f'Ph√¢n ph·ªëi ƒë√°nh gi√° cho s·∫£n ph·∫©m {selected_product}')
    st.pyplot(fig)

    # Hi·ªÉn th·ªã c√°c t·ª´ kh√≥a ph·ªï bi·∫øn nh·∫•t cho ƒë√°nh gi√° t√≠ch c·ª±c v√† ti√™u c·ª±c
    st.write(f"#### T·ª´ kh√≥a ph·ªï bi·∫øn cho s·∫£n ph·∫©m {selected_product}")

    # Hi·ªÉn th·ªã t·ª´ kh√≥a t√≠ch c·ª±c
    most_popular_positive = filtered_review_data['most_popular_positive_word'].iloc[0]
    st.metric("T·ª´ kh√≥a t√≠ch c·ª±c ph·ªï bi·∫øn", most_popular_positive, "‚ú®", delta_color="normal")

    # Hi·ªÉn th·ªã t·ª´ kh√≥a ti√™u c·ª±c
    most_popular_negative = filtered_review_data['most_popular_negative_word'].iloc[0]
    st.metric("T·ª´ kh√≥a ti√™u c·ª±c ph·ªï bi·∫øn", most_popular_negative, "‚ö†Ô∏è", delta_color="inverse")


# Added product search functionality
elif choice == 'Product Search for Customers':
    st.subheader("Product Search Customers")
    
    product_code = st.text_input("Enter Product Code:")
    
    if st.button("T√¨m ki·∫øm"):
        if product_code:            
            product_info = search_product_by_code(product_code)
            print_product_info(product_info)

            if product_info:
                st.title(f"üì¶ {product_info['ten_san_pham']}")

                # Product info in a nice box
                st.info(f"üì¶ {product_info['mo_ta']}")
                
                # Rating with stars
                rating = float(product_info['avg_so_sao'])
                st.markdown(f"### ‚≠ê ƒê√°nh gi√°: {rating:.1f}/5.0")

                left_col, right_col = st.columns(2)

                # Left column: Pie Chart
                with right_col:
                    fig, ax = plt.subplots(figsize=(6, 4))
                    reviews = [int(product_info['total_positive']), int(product_info['total_negative'])]
                    labels = ['Positive', 'Negative']
                    colors = ['#2ecc71', '#e74c3c']
                    ax.pie(reviews, labels=labels, autopct='%1.1f%%', colors=colors)
                    plt.title('Ph√¢n ph·ªëi ƒë√°nh gi√°')
                    st.pyplot(fig)

                # Right column: Review Statistics
                with left_col:
                    with st.expander("üìä Th·ªëng k√™ ƒë√°nh gi√°", expanded=True):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ƒê√°nh gi√° t√≠ch c·ª±c", product_info['total_positive'], "üëç", delta_color="normal")
                        with col2:
                            st.metric("ƒê√°nh gi√° ti√™u c·ª±c", product_info['total_negative'], "üëé", delta_color="inverse")


                # Most used words
                with st.expander("üìù T·ª´ ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t", expanded=True):
                    st.metric("T·ª´ kho√° t√≠ch c·ª±c ph·ªï bi·∫øn", product_info['most_popular_positive_word'], "‚ú®", delta_color="normal")
                    st.metric("T·ª´ kho√° ti√™u c·ª±c ph·ªï bi·∫øn", product_info['most_popular_negative_word'], "‚ö†Ô∏è", delta_color="inverse")
                  
            else:
                st.error("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m!")
        else:
            st.warning("H√£y nh·∫≠p m√£ s·∫£n ph·∫©m")